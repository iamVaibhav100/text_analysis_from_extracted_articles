{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HgyI5wwogceyi7eO8VEQ1KYWUcj5VHWf",
      "authorship_tag": "ABX9TyNUqV8wjeTdksQZH7MdqTro",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamVaibhav100/text_analysis_from_extracted_articles/blob/main/blackcoffer_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install textstat"
      ],
      "metadata": {
        "id": "xQH2npdI5bCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install textblob"
      ],
      "metadata": {
        "id": "zNcKh8E7Cj0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCXi2GivsXJ2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textstat.textstat import textstat\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "hkHKvPSyGP-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the Excel sheet\n",
        "df = pd.read_excel(\"drive/MyDrive/blackcoffer_assignment/Input.xlsx\")"
      ],
      "metadata": {
        "id": "YbQPG3_vvDFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}\n",
        "# # Iterate through each row of the sheet\n",
        "# for index, row in df.iterrows():\n",
        "#     url_id = row['URL_ID']\n",
        "#     url = row['URL']\n",
        "\n",
        "#     # Make a request to the website\n",
        "#     page = requests.get(url, headers=headers)\n",
        "\n",
        "#     # Create a BeautifulSoup object to parse the HTML\n",
        "#     soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "#     # Extract the title and text of the article\n",
        "#     title_h1 = soup.find('h1')\n",
        "#     text_div = soup.find('div', class_='td-post-content')\n",
        "#     if text_div and title_h1:\n",
        "#         text = text_div.text\n",
        "#         title = title_h1.text\n",
        "#     else:\n",
        "#         text = ''\n",
        "#         title = 'Not found!'\n",
        "\n",
        "#     # Write the title and text to a new file with the file name being the URL_ID\n",
        "#     with open(f\"drive/MyDrive/blackcoffer_assignment/extracted/{url_id}.txt\", \"w\") as file:\n",
        "#         file.write(title + '\\n\\n')\n",
        "#         file.write(text)"
      ],
      "metadata": {
        "id": "tpqPwzVovb6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the stopwords from the folder\n",
        "import codecs\n",
        "stopwords = set()\n",
        "stopwords_folder = \"drive/MyDrive/blackcoffer_assignment/stop_words\"\n",
        "for file in os.listdir(stopwords_folder):\n",
        "    if file.endswith(\".txt\"):\n",
        "        with codecs.open(os.path.join(stopwords_folder, file), \"r\", encoding='utf-8', errors='ignore') as f:\n",
        "            stopwords.update(f.read().lower().split())"
      ],
      "metadata": {
        "id": "yK0yYW8zzKmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(stopwords)[:5], len(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1frDzeLt6i77",
        "outputId": "51f7b63a-45da-405b-ff7c-d0fac7366ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['padgett', 'mckee', 'warden', 'bradford', 'kohler'], 12754)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the positive and negative words\n",
        "positive_words = set()\n",
        "with codecs.open(\"drive/MyDrive/blackcoffer_assignment/dictionary/positive-words.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
        "    positive_words.update(f.read().split())\n",
        "negative_words = set()\n",
        "with codecs.open(\"drive/MyDrive/blackcoffer_assignment/dictionary/negative-words.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
        "    negative_words.update(f.read().split())"
      ],
      "metadata": {
        "id": "4gkYoJSD0VvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(positive_words)[:5], list(negative_words)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsdfp0hI6oqb",
        "outputId": "c94289e4-58aa-41f3-83b6-63fd06ada187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['beneficial', 'enthral', 'courtly', 'eye-catching', 'humorous'],\n",
              " ['forfeit', 'repudiation', 'wripped', 'misguidance', 'strangely'])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the dataframe to store the results\n",
        "output = pd.DataFrame(columns=['URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'])"
      ],
      "metadata": {
        "id": "yAcbiM-C6ElA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "RBZuLlVtCpUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# article_names = [\"100.0.txt\", \"101.0.txt\", \"103.0.txt\", \"104.0.txt\", \"105.0.txt\"]\n",
        "# for file in article_names:\n",
        "#     if file.endswith(\".txt\"):\n",
        "#         with open(os.path.join(\"drive/MyDrive/blackcoffer_assignment/extracted\", file), \"r\") as f:\n",
        "#             text = f.read()\n",
        "#     # Clean the text by removing stopwords\n",
        "#     text = \" \".join([word for word in word_tokenize(text) if word.lower() not in stopwords])\n",
        "#     text = text.lower()\n",
        "\n",
        "#     positive_score = 0\n",
        "#     negative_score = 0\n",
        "#     polarity_score = 0\n",
        "#     # Perform sentiment analysis using nltk library\n",
        "#     sid = SentimentIntensityAnalyzer()\n",
        "#     ss = sid.polarity_scores(text)\n",
        "#     positive_score = ss['pos']\n",
        "#     negative_score = ss['neg']\n",
        "#     polarity_score = ss['compound']\n",
        "\n",
        "#     # Perform TextBlob analysis\n",
        "#     subjectivity_score = 0\n",
        "#     tb = TextBlob(text)\n",
        "#     subjectivity_score = tb.sentiment.subjectivity\n",
        "\n",
        "#     # Tokenize the text\n",
        "#     words = word_tokenize(text)\n",
        "\n",
        "#     # Remove the stop words from the text\n",
        "#     filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "#     text = ' '.join(filtered_words)\n",
        "\n",
        "#     # Lemmatize the words\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "#     # Calculate the average sentence length\n",
        "#     avg_sentence_length = 0\n",
        "#     sentences = sent_tokenize(text)\n",
        "#     avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
        "\n",
        "#     complex_word_count = 0\n",
        "#     percentage_of_complex_words = 0\n",
        "#     for word in word_tokenize(text):\n",
        "#         if textstat.syllable_count(word) >= 3:\n",
        "#                 complex_word_count += 1\n",
        "#     percentage_of_complex_words = (complex_word_count / len(word_tokenize(text))) * 100\n",
        "\n",
        "#     word_count = len(word_tokenize(text))\n",
        "#     syllable_per_word = textstat.syllable_count(text) / word_count\n",
        "\n",
        "#     fog_index = 0\n",
        "#     fog_index = 0.4 * ((avg_sentence_length + percentage_of_complex_words))\n",
        "\n",
        "#     avg_number_of_words_per_sentence = 0\n",
        "#     avg_words_per_sentence = len(word_tokenize(text)) / len(sentences)\n",
        "\n",
        "#     personal_pronouns = [\"I\", \"me\", \"mine\", \"my\", \"we\", \"us\", \"our\", \"ours\"]\n",
        "#     personal_pronoun_count = 0\n",
        "#     for word in word_tokenize(text):\n",
        "#         if word.lower() in personal_pronouns:\n",
        "#             personal_pronoun_count += 1\n",
        "\n",
        "#     avg_word_length = sum(len(word) for word in word_tokenize(text)) / len(word_tokenize(text))\n"
      ],
      "metadata": {
        "id": "6wd6xoxj-PdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in os.listdir(\"drive/MyDrive/blackcoffer_assignment/extracted\"):\n",
        "    if file.endswith(\".txt\"):\n",
        "        with open(os.path.join(\"drive/MyDrive/blackcoffer_assignment/extracted\", file), \"r\") as f:\n",
        "            text = f.read()\n",
        "            # Clean the text by removing stopwords\n",
        "            text = \" \".join([word for word in word_tokenize(text) if word.lower() not in stopwords])\n",
        "            text = text.lower()\n",
        "\n",
        "            # Clean the text by removing stopwords\n",
        "            text = \" \".join([word for word in word_tokenize(text) if word.lower() not in stopwords])\n",
        "            text = text.lower()\n",
        "\n",
        "            positive_score = 0\n",
        "            negative_score = 0\n",
        "            polarity_score = 0\n",
        "            # Perform sentiment analysis using nltk library\n",
        "            sid = SentimentIntensityAnalyzer()\n",
        "            ss = sid.polarity_scores(text)\n",
        "            positive_score = ss['pos']\n",
        "            negative_score = ss['neg']\n",
        "            polarity_score = ss['compound']\n",
        "\n",
        "            # Perform TextBlob analysis\n",
        "            subjectivity_score = 0\n",
        "            tb = TextBlob(text)\n",
        "            subjectivity_score = tb.sentiment.subjectivity\n",
        "\n",
        "            # Tokenize the text\n",
        "            words = word_tokenize(text)\n",
        "\n",
        "            # Remove the stop words from the text\n",
        "            filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "            text = ' '.join(filtered_words)\n",
        "\n",
        "            # Lemmatize the words\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "            # Calculate the average sentence length\n",
        "            avg_sentence_length = 0\n",
        "            sentences = sent_tokenize(text)\n",
        "            avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
        "\n",
        "            complex_word_count = 0\n",
        "            percentage_of_complex_words = 0\n",
        "            for word in word_tokenize(text):\n",
        "                if textstat.syllable_count(word) >= 3:\n",
        "                        complex_word_count += 1\n",
        "            percentage_of_complex_words = (complex_word_count / len(word_tokenize(text))) * 100\n",
        "\n",
        "            word_count = len(word_tokenize(text))\n",
        "            syllable_per_word = textstat.syllable_count(text) / word_count\n",
        "\n",
        "            fog_index = 0\n",
        "            fog_index = 0.4 * ((avg_sentence_length + percentage_of_complex_words))\n",
        "\n",
        "            avg_number_of_words_per_sentence = 0\n",
        "            avg_words_per_sentence = len(word_tokenize(text)) / len(sentences)\n",
        "\n",
        "            personal_pronouns = [\"I\", \"me\", \"mine\", \"my\", \"we\", \"us\", \"our\", \"ours\"]\n",
        "            personal_pronoun_count = 0\n",
        "            for word in word_tokenize(text):\n",
        "                if word.lower() in personal_pronouns:\n",
        "                    personal_pronoun_count += 1\n",
        "\n",
        "            avg_word_length = sum(len(word) for word in word_tokenize(text)) / len(word_tokenize(text))\n",
        "\n",
        "    output.append([file, positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "            avg_sentence_length, percentage_of_complex_words, fog_index, avg_words_per_sentence,\n",
        "            complex_word_count, word_count, syllable_per_word, personal_pronoun_count, avg_word_length])"
      ],
      "metadata": {
        "id": "ZYQPnqHH5Lw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0TF8RLfXYJOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.to_csv(\"drive/MyDrive/blackcoffer_assignment/results/text_analysis.csv\", index=False)"
      ],
      "metadata": {
        "id": "wBdG-uMEP2ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aQv1aualYBxa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}